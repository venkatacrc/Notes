{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_intro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNqXgVECzFk9fmaZRDNEkAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatacrc/Notes/blob/master/AWS/ML_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdfftABLFriw",
        "colab_type": "text"
      },
      "source": [
        "#ML Building Blocks: Services and Terminology\n",
        "\n",
        "##Introduction to AWS Machine Learning Services\n",
        "\n",
        "Machine Learning at Amazon\n",
        "* Product Recommendations\n",
        "* Robotics Path Optimization\n",
        "* Alexa Natural Language Understanding and Automated Speech Recognition\n",
        "* Computer Vision Prime Air and in the retail experience with Amazon Go.\n",
        "\n",
        "Machine Learning Stack\n",
        "* Frameworks and Infrastructure\n",
        "  * Apache MXNet\n",
        "  * Caffe & Caffe2\n",
        "  * Tensorflow\n",
        "\n",
        "* Machine learning platforms\n",
        "  * Apache Spark on Amazon EMR (provides access to Presto, Spark, Hive, Pig)\n",
        "  * Spark ML\n",
        "* API-driven services\n",
        "  * Amazon Recognition\n",
        "      * Image Recognition\n",
        "  * Amazon Polly\n",
        "      * Text to Speech\n",
        "  * Amazon Lex\n",
        "      * Conversational interfaces\n",
        "      * Same tech as Alexa\n",
        "* Amazon S3 used as a Data Lake\n",
        "  * Data Analytics\n",
        "    * Amazon Athena\n",
        "    * Amazon Redshift\n",
        "    * Amazon Redshift Spectrum\n",
        "* Models\n",
        "* Optimized Instance & Machine Images\n",
        "\n",
        "## Machine Learning Terminology and Process\n",
        "\n",
        "* Common Machine Learning Terminology\n",
        "  * Training\n",
        "    * Training Dataset\n",
        "    * Test Dataset\n",
        "  * Model\n",
        "  * Prediction (Inference)\n",
        "* The Machine Learning Process\n",
        "  * Feature Engineering\n",
        "    * Converts raw data into a higher representation\n",
        "  * Numeric Value Binnning\n",
        "    * To introduce non-linearity into linear models, intelligently break up continuous values using **binning**.\n",
        "  * Quadratic Features\n",
        "    * Derive new non-linear features by combining feature pairs.\n",
        "  * Non-Linear Feature Transformations\n",
        "    * For numeric features:\n",
        "      * Log, polynomial power of target variable, feature values - may ensure a more \"linear dependence\" with output variable\n",
        "      * Product/ration of feature values\n",
        "    * Tree path features: use leaves of decision tree as features:\n",
        "  * Domain-Specific Transformations\n",
        "    * Text Features:\n",
        "      * Stop-words removal/Stemming\n",
        "      * Loweracasing, punctuation removal\n",
        "      * Cutting off very high/low prcentiles\n",
        "      * TF-IDF (Term Frequency - Inverse Document Frequency) normalization\n",
        "    * Web-page features:\n",
        "      * Multiple fields of text: URL, in/out anchor text, title, frames, body, presence of certain HTML elements(tables/images)\n",
        "      * Relative style (italics/bold, font-size) & positioning\n",
        "  * Parameter Tuning\n",
        "    * Loss Function\n",
        "      * Square: regression, classification\n",
        "      * Hinge: classification only, more robust to outliers\n",
        "      * Logistic: classification only, better for skewed class distributions\n",
        "    * Regularization\n",
        "      * Prevent overfitting by constraining weights to be small\n",
        "    * Learning Parameters(e.g. decay rate)\n",
        "      * Decaying too aggressively - algorithm never reaches optimum\n",
        "      * Decaying too slowly - algorithm bounces around, never converges to optimum\n",
        "  * Evaluation Metrics\n",
        "    * Metrics when regression is used for predicting target values\n",
        "      * RMSE\n",
        "      * MAPE (Mean Absolute Percent Error)\n",
        "      * $R^2$: How much better is the model compared to just picking the best constant? $R^2 = 1 - \\frac{MSE}{Variance}$\n",
        "    * For Classification \n",
        "      * Confusion Matrix\n",
        "      * ROC Curve\n",
        "      * Precision-Recall\n",
        "\n",
        "![](https://drive.google.com/uc?id=15yX8ruenW7KRvn_ClpmxzOTuv3A3ymNk)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}