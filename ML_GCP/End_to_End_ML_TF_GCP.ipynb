{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "End-to-End-ML-TF-GCP.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatacrc/Notes/blob/master/ML_GCP/End_to_End_ML_TF_GCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Km43xsqqc4",
        "colab_type": "text"
      },
      "source": [
        "##End to End Machine Learning with TensorFlow on GCP\n",
        "Source: [End to End Machine Learning with TensorFlow on GCP](https://www.coursera.org/learn/end-to-end-ml-tensorflow-gcp)\n",
        "\n",
        "1. Explore, visualize a dataset\n",
        "1. Create sampled dataset\n",
        "1. Develop a Tensorflow model\n",
        "1. Create training and evaluaton datasets\n",
        "1. Execute training\n",
        "1. Deploy prediction service\n",
        "1. Invoke ML predictions\n",
        "\n",
        "Distributed Tensorflow on Cloud ML Engine\n",
        "\n",
        "Components | Description\n",
        "---|---\n",
        "tf.estimator| High level API for distributed training\n",
        "tf.layers, tf.losses, tf.metrics| Components useful when building cusom NN models\n",
        "Core Tensorflow(Python) | Python API gives you full control \n",
        "Core Tensorflow(C++) | C++ API is quite low level\n",
        "CPU, GPU, TPU, Android| TF runs on different hardware\n",
        "\n",
        "When the data can't fit into memory, batching and distribution become important.\n",
        "\n",
        "Main parts:\n",
        "1. Big Data\n",
        "1. Effective feature engineering\n",
        "1. Model Architectures\n",
        "\n",
        "Need autosclaing based on number of prediction client queries.\n",
        "\n",
        "Inputs $\\Rightarrow$ Pre-processing $\\Rightarrow$ Feture Engineering $\\Rightarrow$ Train Model(Hyper-parameter tuning) $\\Rightarrow$ TF Model $\\Rightarrow$ Deploy Web Application $\\leftarrow$ REST API call with input variables $-$ Clients\n",
        "\n",
        "**Training-Serving Skew**: The difference between what it was trainined on and what it's being presented at prediction time.\n",
        "\n",
        "Cloud ML provides repeatable, scalable, and tuned.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1O_ncl8texv1kn-OK847NzXeBPUFAse6H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuzDEpksk19g",
        "colab_type": "text"
      },
      "source": [
        "###Explore the data\n",
        "\n",
        "https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu\n",
        "\n",
        "BigQuery is a serverless data warehouse\n",
        "1. Interative analysis of petabyte scale databases\n",
        "1. Familiar, SQL 2011 query language and functions\n",
        "1. Many ways to ingest, transform, load, export data to/from BigQuery\n",
        "1. Nested and repeated fields, user-defined functions (JSON)\n",
        "1. Data storage is inexpensive; queries charged on amount of data processed (or a monthly flat rate)\n",
        "\n",
        "AI Platform Notebooks is the next generation of hosted notebook on GCP\n",
        "\n",
        "CSV Files $\\rightarrow$ Apache Beam $\\rightarrow$ Pandas DF $\\rightarrow$ TF\n",
        "\n",
        "Cloud Storage $\\rightarrow$ Cloud Dataflow $\\rightarrow$ Cloud AI Platform\n",
        "\n",
        "####Lab 1\n",
        "Exploring a BigQuery dataset to find features to use in an ML model\n",
        "```\n",
        "# Create SQL query using natality data after the year 2000\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  weight_pounds,\n",
        "  is_male,\n",
        "  mother_age,\n",
        "  plurality,\n",
        "  gestation_weeks,\n",
        "  FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\n",
        "FROM\n",
        "  publicdata.samples.natality\n",
        "WHERE year > 2000\n",
        "\"\"\"\n",
        "\n",
        "# Call BigQuery and examine in dataframe\n",
        "from google.cloud import bigquery\n",
        "df = bigquery.Client().query(query + \" LIMIT 100\").to_dataframe()\n",
        "df.head()\n",
        "```\n",
        "```\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "    is_male,\n",
        "    COUNT(1) AS num_babies,\n",
        "    AVG(weight_pounds) AS avg_wt\n",
        "FROM\n",
        "    publicdata.samples.natality\n",
        "WHERE \n",
        "    year > 2000\n",
        "GROUP BY\n",
        "    is_male\n",
        "\"\"\"\n",
        "\n",
        "# Call BigQuery and examine in dataframe\n",
        "from google.cloud import bigquery as bq\n",
        "df = bq.Client().query(query).to_dataframe()\n",
        "df.head()\n",
        "\n",
        "df.plot(x='is_male', y='num_babies', kind='bar')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry4M_DFoBJUX",
        "colab_type": "text"
      },
      "source": [
        "###Create the dataset\n",
        "\n",
        "What makes a feature \"good\"?\n",
        "1. Be related to the objective\n",
        "1. Be known at prediction-time\n",
        "1. Be numeric with meaningful magnitude\n",
        "1. Have enough examples\n",
        "1. Bring human insight to problem\n",
        "\n",
        "Data leakage can happen when similar data exists in training and testing sets\n",
        "\n",
        "Developing the ML model software on the entire dataset can be expensive; you want to develop on a smaller sample.\n",
        "```\n",
        "SELECT\n",
        "  date,\n",
        "  airline,\n",
        "  departure_airport,\n",
        "  departure_schedule,\n",
        "  arrival_airport,\n",
        "  arrival_delay\n",
        "FROM\n",
        "  `bigquery-samples.airline_ontime_data.flights`\n",
        "WHERE\n",
        "  MOD(ABS(FARM_FINGERPRINT(date)), 10) < 8 and RAND() < 0.01 # keep only 1% of the dataset\n",
        "```\n",
        "In ML, you could train using all your data and decide not to hold out a test set and still get a good model.\n",
        "\n",
        "Hashing and modulo operators for creating ML datasets helps to create repeatable training and test data sets.\n",
        "\n",
        "###Lab2\n",
        "* Sampling a BigQuery dataset to create datasets for ML\n",
        "* Preprocessing with Pandas\n",
        "\n",
        "```\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHuaNsrZbXPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['BUCKET'] = 'qwiklabs-gcp-04-0ab29bbe8108' #BUCKET\n",
        "os.environ['PROJECT'] = 'qwiklabs-gcp-04-0ab29bbe8108' #PROJECT\n",
        "os.environ['REGION'] = 'us-east1' #REGION\n",
        "\n",
        "%%bash\n",
        "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
        "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
        "fi\n",
        "\n",
        "# Create SQL query using natality data after the year 2000\n",
        "from google.cloud import bigquery\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  weight_pounds,\n",
        "  is_male,\n",
        "  mother_age,\n",
        "  plurality,\n",
        "  gestation_weeks,\n",
        "  FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\n",
        "FROM\n",
        "  publicdata.samples.natality\n",
        "WHERE year > 2000\n",
        "\"\"\"\n",
        "\n",
        "# Call BigQuery but GROUP BY the hashmonth and see number of records for each group to enable us to get the correct train and evaluation percentages\n",
        "df = bigquery.Client().query(\"SELECT hashmonth, COUNT(weight_pounds) AS num_babies FROM (\" + query + \") GROUP BY hashmonth\").to_dataframe()\n",
        "print(\"There are {} unique hashmonths.\".format(len(df)))\n",
        "df.head()\n",
        "\n",
        "# Added the RAND() so that we can now subsample from each of the hashmonths to get approximately the record counts we want\n",
        "trainQuery = \"SELECT * FROM (\" + query + \") WHERE ABS(MOD(hashmonth, 4)) < 3 AND RAND() < 0.0005\"\n",
        "evalQuery = \"SELECT * FROM (\" + query + \") WHERE ABS(MOD(hashmonth, 4)) = 3 AND RAND() < 0.0005\"\n",
        "traindf = bigquery.Client().query(trainQuery).to_dataframe()\n",
        "evaldf = bigquery.Client().query(evalQuery).to_dataframe()\n",
        "print(\"There are {} examples in the train dataset and {} in the eval dataset\".format(len(traindf), len(evaldf)))\n",
        "\n",
        "traindf.head()\n",
        "\n",
        "# Let's look at a small sample of the training data\n",
        "traindf.describe()\n",
        "\n",
        "# It is always crucial to clean raw data before using in ML, so we have a preprocessing step\n",
        "import pandas as pd\n",
        "def preprocess(df):\n",
        "  # clean up data we don't want to train on\n",
        "  # in other words, users will have to tell us the mother's age\n",
        "  # otherwise, our ML service won't work.\n",
        "  # these were chosen because they are such good predictors\n",
        "  # and because these are easy enough to collect\n",
        "  df = df[df.weight_pounds > 0]\n",
        "  df = df[df.mother_age > 0]\n",
        "  df = df[df.gestation_weeks > 0]\n",
        "  df = df[df.plurality > 0]\n",
        "  \n",
        "  # modify plurality field to be a string\n",
        "  twins_etc = dict(zip([1,2,3,4,5],\n",
        "                   ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)']))\n",
        "  df['plurality'].replace(twins_etc, inplace=True)\n",
        "  \n",
        "  # now create extra rows to simulate lack of ultrasound\n",
        "  nous = df.copy(deep=True)\n",
        "  nous.loc[nous['plurality'] != 'Single(1)', 'plurality'] = 'Multiple(2+)'\n",
        "  nous['is_male'] = 'Unknown'\n",
        "  \n",
        "  return pd.concat([df, nous])\n",
        "\n",
        "traindf.head()# Let's see a small sample of the training data now after our preprocessing\n",
        "traindf = preprocess(traindf)\n",
        "evaldf = preprocess(evaldf)\n",
        "traindf.head()\n",
        "\n",
        "traindf.tail()\n",
        "\n",
        "# Describe only does numeric columns, so you won't see plurality\n",
        "traindf.describe()\n",
        "\n",
        "traindf.to_csv('train.csv', index=False, header=False)\n",
        "evaldf.to_csv('eval.csv', index=False, header=False)\n",
        "\n",
        "%%bash\n",
        "wc -l *.csv\n",
        "head *.csv\n",
        "tail *.csv\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puzxQN8Ii6Bj",
        "colab_type": "text"
      },
      "source": [
        "###Build the Model\n",
        "\n",
        "TensorFlow is an open-source high-performance library for numerical computation that uses directed graphs.\n",
        ">nodes = mathematical operations\n",
        "\n",
        ">edges = arrays of data\n",
        "\n",
        "Working with Estimator API\n",
        "\n",
        "Set up ML model\n",
        "1. Regression or Classification?\n",
        "1. What is the label?\n",
        "1. What are the featuers?\n",
        "\n",
        "Carry out ML steps\n",
        "1. Train the model\n",
        "1. Evaluate the model\n",
        "1. Predict with the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-K2g8qWIQXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Structure of an Estimator API ML model\n",
        "import tensorflow as tf\n",
        "#Define input feature columns\n",
        "featcols = [tf.feature_column.numeric_column(\"sq_footage\")]\n",
        "\n",
        "#Encoding categorical data to supply to a DNN\n",
        "# A linear model can handle the sparse data directly\n",
        "#tf.feature_column.categorical_column_with_vocabulary_list('zipcode', vocabulary_list = ['94582', '94107'])\n",
        "#tf.feature_column.categorical_column_with_identity('stateId', num_buckets = 50)\n",
        "# Option to make sparse column dense is by one-hot encoding using indicator_column\n",
        "#tf.feature_column.indicator_column(my_categorical_column)\n",
        "\n",
        "#Instantiate Linear Regression Model\n",
        "model = tf.estimator.LinearRegression(featcols, './model_trained')\n",
        "#Train\n",
        "def train_input_fn():\n",
        "  ...\n",
        "  return features, labels\n",
        "model.train(train_input_fn, steps=100)\n",
        "#Predict\n",
        "def pred_input_fn():\n",
        "  ...\n",
        "  return features\n",
        "out = model.predict(pred_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEC5ht_pbnWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To read CSV files, create a TextLineDataset giving it a function to decode the CSV into features, labels\n",
        "CSV_COLUMNS = ['sqfootage', 'city', 'amount']\n",
        "LABEL_COLUMN = 'amount'\n",
        "DEFAULTS = [[0.0], ['na'], [0.0]]\n",
        "\n",
        "def read_dataset(filename, mode, batch_size=512):\n",
        "  def decode_csv(value_common):\n",
        "    columns = tf.decode_csv(value_common, record_defaults=DEFAULTS)\n",
        "    features = dict(zip(CSV_COLUMNS, columns))\n",
        "    label = features.pop(LABEL_COLUMN)\n",
        "    return features, label\n",
        "  dataset = tf.data.TextLineDataset(filename).map(decode_csv)\n",
        "  ...\n",
        "  return ...\n",
        "\n",
        "#Shuffling is important for distributed training\n",
        "\n",
        "def read_dataset(filename, mode, batch_size=512):\n",
        "  ...\n",
        "  dataset = tf.data.TextLineDataset(filename).map(decode_csv)\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    num_epochs = None # to read indefinitely\n",
        "    dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
        "  else:\n",
        "    num_epochs = 1 #to evaluate read data once end-of-input after this\n",
        "  #read data on batches\n",
        "  dataset= dataset.repeat(num_epochs).batch(batch_size)\n",
        "\n",
        "  return dataset.make_one_shot_iterator().get_next()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFAdzwpTdzKO",
        "colab_type": "text"
      },
      "source": [
        "Estimator API comes with a method that handles distributed training and evaluation\n",
        "* Distribute the graph\n",
        "* Share variables\n",
        "* Evaluate occassionally\n",
        "* Handle machine failures\n",
        "* Create checkpoint files\n",
        "* Recover from failures\n",
        "* Save summaries for Tensorboard\n",
        "\n",
        "TrainSpec consists of the things that used to be passed into the train() method\n",
        "\n",
        "####Think \"Steps\", not 'epochs\" with production-ready, distributed models.\n",
        "\n",
        "1. Gradient updates from slow workers could get ignored\n",
        "1. When retraining a model with fresh dta, we'll resume from earlier number of steps(and corresponding hyper-parameters)\n",
        "\n",
        "####EvalSpec controls the evaluation and the checkpointing of the model because they happen at the same time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPcMwxvjjYji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = tf.estimator.LinearRegressor(model_dir=output_dir,\n",
        "                                         feature_columns=feature_cols)\n",
        "...\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "    input_fn=read_dataset('gs://.../train*',\n",
        "                          mode=tf.contrib.learn.ModeKeys.TRAIN),\n",
        "    max_steps=num_train_steps)\n",
        "...\n",
        "exporter = ... #used for checkpointing\n",
        "eval_spec=tf.estimator.EvalSpec(\n",
        "    input_fn=('gs://.../valid*', \n",
        "              mode=tf.contrib.learn.ModeKeys.EVAL),\n",
        "    steps = None,\n",
        "    short_delay_secs=60, #start evaluating after N seconds\n",
        "    throttle_secs=600,   #evaluate every N seconds\n",
        "    exporters=exporter)\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator,\n",
        "                                train_spec,\n",
        "                                eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOwYhM3InbzC",
        "colab_type": "text"
      },
      "source": [
        "Linear models are better at handling sparse, independent features. Helps memorize features better.\n",
        "\n",
        "DNN models are better at handling dense, correlated features. Helps to decorrelate the inputs and generalize better by capturing the relationship between the dense inputs and the labels.\n",
        "\n",
        "Wide and Deep Models let you handle both sparse and dense inputs well. By using wide and deep models you get to tradeoff relevance in case sparse and diversity in case of dense inputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNgOwi09pC8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Wide-and-deep network in Estimator API\n",
        "model = tf.estimator.DNNLinearCombinedClassifier(\n",
        "    model_dir=...,\n",
        "    linear_feature_columns=wide_columns,\n",
        "    dnn_feature_columns=deep_columns,\n",
        "    dnn_hidden_units=[100,50]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZdH0vmDzgGv",
        "colab_type": "text"
      },
      "source": [
        "###Lab3\n",
        "\n",
        "* Reading the dataset and creating the features\n",
        "* building the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0AmdTtLKOza",
        "colab_type": "text"
      },
      "source": [
        "## Productionalize ML Pipelines Elastically with Cloud Dataflow\n",
        "\n",
        "Why Cloud Dataflow are:\n",
        "* Executes data processing pipelines written using Apache Beam API\n",
        "* Process large amounts of data in parallel\n",
        "* Supports Steaming and Batch jobs for training and serving\n",
        "* Changes the number of servers that will run your pipeline elastically, depending on the amount of data that your pipeline needs to process\n",
        "\n",
        "Apache Beam is a unified model: \n",
        "* Contraction **b**atch + str**eam** = **beam**\n",
        "* for defining both batch and streaming data parallel processing pipelines\n",
        "* a set of language specific SDKs for constructing pipelines and runners for executing them on distributed processing backends\n",
        "\n",
        "What is a pipeline:\n",
        "A sequence of steps that change data from one format to another\n",
        "\n",
        "Big Query $\\Rightarrow$ Google Cloud Storage\n",
        "\n",
        "Open-Source API(Apache Beam) can be executed on Flink, Spark, etc.\n",
        "\n",
        "Parallel Map tasks (autoscaled by execution framework)\n",
        "```\n",
        "def Transform(line):\n",
        "  return (count_words(line), 1)\n",
        "\n",
        "# ignore lines fewer than 10 words\n",
        "def Filter(key, values):\n",
        "  return key > 10\n",
        "\n",
        "p = beam.Pipeline()\n",
        "(p\n",
        "  | beam.io.ReadFromText('gs://..')\n",
        "  | beam.Map(Transform)\n",
        "  | beam.GroupByKey()\n",
        "  | beam.FlatMap(Filter)\n",
        "  | beam.io.WriteToText('gs://...')\n",
        "}\n",
        "p.run();\n",
        "\n",
        "p = beam.Pipeline()\n",
        "(p\n",
        "  | beam.io.ReadStringsFromPubSub('project/topic')\n",
        "  | beam.WindowInto(SlidingWindows(60))\n",
        "  | beam.Map(Transform)\n",
        "  | beam.GroupByKey()\n",
        "  | beam.FlatMap(Filter)\n",
        "  | beam.io.WriteToBigQuery(table)\n",
        "}\n",
        "p.run();\n",
        "```\n",
        "\n",
        "$ \n",
        "\\left.\\begin{array}{rcl}\n",
        "{Cloud Pub/Sub} \\\\\n",
        "{Cloud Storage}\n",
        "\\end{array}\\right\\} \\rightarrow Cloud Dataflow \\rightarrow \\left\\{ \n",
        "\\begin{array}{rcl}\n",
        "{BigQuery}\\\\ \n",
        "{Cloud Pub/Sub} \\\\\n",
        "{Cloud Storage}\n",
        "\\end{array}\\right.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3mOmFPIAhPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#etl.py\n",
        "import apache_beam as beam\n",
        "def transform_data(rowdict):\n",
        "  import copy\n",
        "  result = copy.deepcopy(rowdict)\n",
        "  if rowdict['a'] > 0:\n",
        "    result['c'] = result['a'] * result['b']\n",
        "    yield ','.join([str(result[k]) if k in result else 'None' for k in ['a', 'b', 'c']])\n",
        "if __name__ = '__main__':\n",
        "  p = beam.Pipeline(argv=sys.argv)\n",
        "  selquery = 'SELECT a,b FROM someds.sometable'\n",
        "  (p\n",
        "   | beam.io.Read(beam.io.BigQuerySource(query=selquery,use_standard_sql=True)) # read inpu\n",
        "   | beam.Map(transform_data) # do some processing\n",
        "   | beam.io.WriteToText('gs://...') # write output\n",
        "   )\n",
        "  p.run()\n",
        "#Executing the pipeline\n",
        "#Simply running main() runs pipeline locally\n",
        "%python ./etl.py\n",
        "# To run on cloud, specify cloud parameters\n",
        "%python ./etl.py\\\n",
        "    --project=$PROJECT \\\n",
        "    --job_name=myjob \\\n",
        "    --staging_location=gs://$BUCKET/staging/ \\\n",
        "    --temp_location=gs://$BUCKET/staging/ \\\n",
        "    --runner=DataflowRunner #DirectRunner would be local"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfoKH5R4TYPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lab 4 Data Preprocessing\n",
        "import datetime, os\n",
        "\n",
        "def to_csv(rowdict):\n",
        "  # Pull columns from BQ and create a line\n",
        "  import hashlib\n",
        "  import copy\n",
        "  CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks'.split(',')\n",
        "\n",
        "  # Create synthetic data where we assume that no ultrasound has been performed\n",
        "  # and so we don't know sex of the baby. Let's assume that we can tell the difference\n",
        "  # between single and multiple, but that the errors rates in determining exact number\n",
        "  # is difficult in the absence of an ultrasound.\n",
        "  no_ultrasound = copy.deepcopy(rowdict)\n",
        "  w_ultrasound = copy.deepcopy(rowdict)\n",
        "\n",
        "  no_ultrasound['is_male'] = 'Unknown'\n",
        "  if rowdict['plurality'] > 1:\n",
        "    no_ultrasound['plurality'] = 'Multiple(2+)'\n",
        "  else:\n",
        "    no_ultrasound['plurality'] = 'Single(1)'\n",
        "\n",
        "  # Change the plurality column to strings\n",
        "  w_ultrasound['plurality'] = ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)'][rowdict['plurality'] - 1]\n",
        "\n",
        "  # Write out two rows for each input row, one with ultrasound and one without\n",
        "  for result in [no_ultrasound, w_ultrasound]:\n",
        "    data = ','.join([str(result[k]) if k in result else 'None' for k in CSV_COLUMNS])\n",
        "    key = hashlib.sha224(data.encode('utf-8')).hexdigest()  # hash the columns to form a key\n",
        "    yield str('{},{}'.format(data, key))\n",
        "  \n",
        "def preprocess(in_test_mode):\n",
        "  import shutil, os, subprocess\n",
        "  job_name = 'preprocess-babyweight-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
        "\n",
        "  if in_test_mode:\n",
        "      print('Launching local job ... hang on')\n",
        "      OUTPUT_DIR = './preproc'\n",
        "      shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
        "      os.makedirs(OUTPUT_DIR)\n",
        "  else:\n",
        "      print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
        "      OUTPUT_DIR = 'gs://{0}/babyweight/preproc/'.format(BUCKET)\n",
        "      try:\n",
        "        subprocess.check_call('gsutil -m rm -r {}'.format(OUTPUT_DIR).split())\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "  options = {\n",
        "      'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
        "      'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
        "      'job_name': job_name,\n",
        "      'project': PROJECT,\n",
        "      'region': REGION,\n",
        "      'teardown_policy': 'TEARDOWN_ALWAYS',\n",
        "      'no_save_main_session': True,\n",
        "      'max_num_workers': 6\n",
        "  }\n",
        "  opts = beam.pipeline.PipelineOptions(flags = [], **options)\n",
        "  if in_test_mode:\n",
        "      RUNNER = 'DirectRunner'\n",
        "  else:\n",
        "      RUNNER = 'DataflowRunner'\n",
        "  p = beam.Pipeline(RUNNER, options = opts)\n",
        "  query = \"\"\"\n",
        "SELECT\n",
        "  weight_pounds,\n",
        "  is_male,\n",
        "  mother_age,\n",
        "  plurality,\n",
        "  gestation_weeks,\n",
        "  FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\n",
        "FROM\n",
        "  publicdata.samples.natality\n",
        "WHERE year > 2000\n",
        "AND weight_pounds > 0\n",
        "AND mother_age > 0\n",
        "AND plurality > 0\n",
        "AND gestation_weeks > 0\n",
        "AND month > 0\n",
        "    \"\"\"\n",
        "\n",
        "  if in_test_mode:\n",
        "    query = query + ' LIMIT 100' \n",
        "\n",
        "  for step in ['train', 'eval']:\n",
        "    if step == 'train':\n",
        "      selquery = 'SELECT * FROM ({}) WHERE ABS(MOD(hashmonth, 4)) < 3'.format(query)\n",
        "    else:\n",
        "      selquery = 'SELECT * FROM ({}) WHERE ABS(MOD(hashmonth, 4)) = 3'.format(query)\n",
        "\n",
        "    (p \n",
        "     | '{}_read'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query = selquery, use_standard_sql = True))\n",
        "     | '{}_csv'.format(step) >> beam.FlatMap(to_csv)\n",
        "     | '{}_out'.format(step) >> beam.io.Write(beam.io.WriteToText(os.path.join(OUTPUT_DIR, '{}.csv'.format(step))))\n",
        "    )\n",
        "\n",
        "  job = p.run()\n",
        "  if in_test_mode:\n",
        "    job.wait_until_finish()\n",
        "    print(\"Done!\")\n",
        "    \n",
        "preprocess(in_test_mode = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbe43wgsXWls",
        "colab_type": "text"
      },
      "source": [
        "###Distributed Training on the entire dataset\n",
        "Create task.py to parse command line parameters and send to train_and_evaluate\n",
        "\n",
        "Package TensorFlow model as a Python Package\n",
        "Python packages need to contain an __init__.py in every folder.\n",
        "\n",
        "```\n",
        "taxifare/\n",
        "taxifare/PKG-INFO\n",
        "taxifare/setup.cfg\n",
        "taxifare/setup.py\n",
        "taxifare/trainer/\n",
        "taxifare/trainer/__init__.py\n",
        "taxifare/trainer/task.py\n",
        "taxifare/trainer/model.py\n",
        "```\n",
        "Verify that the model works as a Python package\n",
        "```\n",
        "export PYTHONPATH=${PYTHONPATH}:/somedir/babyweight\n",
        "python -m trainer.task \\\n",
        "  --train_data_paths=\"/some/datasets/*train*\" \\\n",
        "  --eval_data_paths=\"/some/datasets/*eval*\" \\\n",
        "  --output_dir=/somedir/output \\\n",
        "  --train_steps=100 --job-dir=/tmp\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6IaP0sEX4AL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#task.py parse command line argument\n",
        "parser.add_argument('--train_data_paths', required=True)\n",
        "parser.add_argument('--train_steps', required=True)\n",
        "\n",
        "#model.py\n",
        "\n",
        "def train_and_evaluate(args):\n",
        "  estimator = tf.estimator.DNNRegressor(\n",
        "      model_dir=args['output_dir'],\n",
        "      feature_columns=feature_cols,\n",
        "      hidden_units=args['hidden_units']\n",
        "  )\n",
        "  train_spec = tf.estimator.TrainSpec(\n",
        "      input_fn=read_dataset(args['train_data_paths'],\n",
        "                            batch_size=args['train_batch_size'],\n",
        "                            mode=tf.contrib.learn.ModeKeys.TRAIN),\n",
        "                            max_steps=args['train_steps']\n",
        "  )\n",
        "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(...)\n",
        "  tf.estimator.train_and_evaluate(estimator,\n",
        "                                  train_spec,\n",
        "                                  eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG-lwNqpbQa9",
        "colab_type": "text"
      },
      "source": [
        "###Train and Deploy with Cloud ML Engine\n",
        "\n",
        "* Use the gcloud command to submit the training job either locally or to the cloud\n",
        "* Carry out distributed training and hyperparameter tuning on Cloud ML Engine\n",
        "  1. change batch size if necessary\n",
        "  1. calculate train steps based on #examples\n",
        "  1. make hyper-parameters commond-line params\n",
        "* Submit the training job on the full dataset and monitor using TensorBoard\n",
        "\n",
        "```\n",
        "gcloud ml-engine local train \\\n",
        "  --module_name=trainer.task \\\n",
        "  --package-path=/somedir/babyweight/trainer \\\n",
        "  -- train data_paths etc.\n",
        "  REST as before\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "  --regin=$REGION \\\n",
        "  --module_name=trainer.task \\\n",
        "  --job-dir=$OUTDIR \\\n",
        "  --staging_bucket=gs://$BUCKET \\\n",
        "  --scale-tier=BASIC \\\n",
        "  REST as before\n",
        "```"
      ]
    }
  ]
}