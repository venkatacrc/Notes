{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WhyIsMyClassifierDiscriminatory.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatacrc/Notes/blob/master/ML_papers/WhyIsMyClassifierDiscriminatory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE1j4N3jI7vn",
        "colab_type": "text"
      },
      "source": [
        "# Why Is My Classifier Discriminatory?\n",
        "\n",
        "Source: [Why Is My Classifier Discriminatory?](https://youtu.be/kLUc1bG2rv4)\n",
        "\n",
        "### Motivation\n",
        "It is **surprisingly easy** to make a discriminatory algorithm even accidentally.\n",
        "\n",
        "Logistic regression showed differences in accuracy by race, especially high errors for asians. \n",
        "\n",
        "This paper includes:\n",
        "* Find the sources of unfairness to guide resource allocation.\n",
        "* Decompose unfairness into bias, variance, and noise.\n",
        "* Demonstrate methods to guide feature augmentation and training data collection to fix unfairness.\n",
        "\n",
        "### What are some of the reasons for unfairness?\n",
        "* Some groups are underrepresented, resulting higher errors.\n",
        "  This is due to **Variance** and can be solved by collecting more data.\n",
        "* Model class ill suited for a particular group. The resulting error is due to **Bias** and can be fixed by changing the model class. \n",
        "* If one group is harder to predict compared to another even with the best model and infinite data. This error is due to noise and recommended to collect more features.\n",
        "\n",
        "### Method\n",
        "Fairness definition:\n",
        "Fairness is defined as a difference between the loss metric between two groups.\n",
        "In the context of **loss functions** like false positive rate, false negative rate, etc.\n",
        "\n",
        "For example, acuracy for data D and prediction $\\hat Y$:\n",
        "\n",
        "$\\gamma_a(\\hat Y, Y, D) := P_D(\\hat Y \\neq Y | A = a)$\n",
        "\n",
        "Formalizing **unfairness as group differences**\n",
        "\n",
        "$\\bar \\Gamma(\\hat Y) := |\\gamma_1 - \\gamma_0|$\n",
        "\n",
        "Note: This method relies on accurate Y labels and focus on algorithmic error.\n",
        "\n",
        "**Theorem 1:** For error over group a given predictor $\\hat Y$:\n",
        "\n",
        "$\\bar\\gamma_a(\\hat Y) = \\bar B_a(\\hat Y) + \\bar V_a(\\hat Y) + \\bar N_a$\n",
        "\n",
        "Note that $\\bar N_a$ indicates the expectation of $N_a$ over X and data D.\n",
        "\n",
        "Accordingly, The expected discrimination level $\\bar \\Gamma(\\hat Y) := |\\gamma_1 - \\gamma_0|$ can be decomposed into differences in bias, differences in variance, and differences in noise.\n",
        "\n",
        "$\\bar\\Gamma = |(\\bar B_1 - \\bar B_0)| + |(\\bar V_1 - \\bar V_0)| + |(\\bar N_1 - \\bar N_0)|$\n",
        "\n",
        "From MIMIC-III clinical data:\n",
        "* Found statistically significant racial differences in zero-one loss\n",
        "* By subsampling data, fitted inverse power loss to estimate the benefit of more data and reducing variance\n",
        "* Using topic modeling, identified subpopulations to gather more features to reduce noise\n",
        "\n",
        "###Conclusion\n",
        "1. For **accurate and fair models** deployed in real world applications, both the **data** and **model** should be considered.\n",
        "1. By using easily implementable fairness checks, the algorithms can be checked for bias, variance, and noise which will guide further efforts to reduce unfairness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z54WjLmUgdly",
        "colab_type": "text"
      },
      "source": [
        "Group-specific Loss\n",
        "\n",
        "> $\\bar \\gamma_a = \\underbrace{\\bar B_a(\\hat Y)}\\limits_{Bias} + \\underbrace{\\bar V_a(\\hat Y)}\\limits_{Variance} + \\underbrace{\\bar N_a} \\limits_{Noise}$\n",
        "\n",
        "Discrimination Level\n",
        "\n",
        ">$\\bar\\Gamma = |(\\bar B_1 - \\bar B_0)| + |(\\bar V_1 - \\bar V_0)| + |(\\bar N_1 - \\bar N_0)|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h2IF5YhqXA2",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}