{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Normalization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+QhmjvGU+YieO5cozpzq2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatacrc/Notes/blob/master/MachineLearning/Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwJ5KOd22fXI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "###Normalizing Activations in a Network\n",
        "\n",
        "[Source](https://www.youtube.com/watch?v=tNIpEZLv_eg)\n",
        "\n",
        "Normalizing input features can speed up learning by turning a learning problem with elongated contours into something that is round and easier for an algorithm like Gradient Descent to optimize.\n",
        "\n",
        "![Logistic Regression Model](https://drive.google.com/uc?id=1gF5_FVYCmFUK_9teC2a4z2gF1vsCrtc_)\n",
        "\n",
        "Given X, \n",
        "\n",
        "$\\mu = \\frac{1}{H}\\sum_{i}X^{(i)}$ (compute mean)\n",
        "\n",
        "$X = X - \\mu$ (subtract mean from inputs)\n",
        "\n",
        "$\\sigma^2 = \\frac{1}{H}\\sum_{i}{X^{(i)}}^2$ (element wise squaring)\n",
        "\n",
        "$X = \\frac{X}{\\sigma^2}$ (divide by variance)\n",
        "\n",
        "![Simplifying the learning problem](https://drive.google.com/uc?id=1znnMB9ljGjsT4CGxMh7wkmK7Sm3-go2Q)\n",
        "\n",
        "### Implementing Batch Norm\n",
        "\n",
        "Given some intermediate values in NN $Z^{(1)},...,Z^{(m)}$\n",
        "\n",
        "For some hidden layer l $Z^{[l](i)}$\n",
        "\n",
        "$\\mu = \\frac{1}{H}\\sum_{i}Z^{(i)}$\n",
        "\n",
        "$\\sigma^2 = \\frac{1}{H}\\sum_{i}{(Z^{(i)} - \\mu)}^2$\n",
        "\n",
        "$Z_{norm}^{(i)} = \\frac{Z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r0Fl1Ill6JLC"
      },
      "source": [
        "\n",
        "###Normalizing Activations in a Netwrok\n",
        "\n",
        "Normalizing inputs to speed up learning\n",
        "\n",
        "Given X, \n",
        "\n",
        "$\\mu = \\frac{1}{H}\\sum_{i}X^{(i)}$\n",
        "\n",
        "$X = X - \\mu$\n",
        "\n",
        "$\\sigma^2 = \\frac{1}{H}\\sum_{i}{X^{(i)}}^2$\n",
        "\n",
        "$X = \\frac{X}{\\sigma^2}$\n",
        "\n",
        "### Implementing Batch Norm\n",
        "\n",
        "Given some intermediate values in NN $Z^{(1)}, $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ob4QSkxE4yQ",
        "colab_type": "text"
      },
      "source": [
        "#Caffe2 Operators\n",
        "\n",
        "##LayerNorm\n",
        "\n",
        "[Source](https://caffe2.ai/docs/operators-catalogue.html#layernorm)\n",
        "\n",
        "Given an input vector,\n",
        " \n",
        " $x \\in [a_0, a_1, …,a_{k-1}, a_k, …, a_{n-1}]$\n",
        "\n",
        "this op treats dimensions $a_k$ through $a_{n-1}$ as feature vectors. For each feature vector, the op contrains the mean and standard deviation. Then, it returns the normalized values  (wrt the feature vector).\n",
        "\n",
        "Note:  This op does not contain the scale and bias terms described in the paper. Follow this op with an FC op to add those. \n",
        "\n",
        "\n",
        "Currently, the op implements:\n",
        "\n",
        "$h = \\frac{1}{\\sigma}(a - \\mu) $\n",
        "\n",
        "$\\mu = \\frac{1}{H}\\sum_{i=1}^{H} a_i $\n",
        "\n",
        "$\\sigma = \\sqrt{\\frac{1}{H}\\sum_{i=1}^{H}(a_i - \\mu)^2}$\n",
        "\n",
        "where H is the number of hidden units (i.e. product of dimensions from 'axis' to the end.)\n",
        "\n",
        "###Interface\n",
        "- Arguments|-\n",
        "---|---\n",
        "axis| (int) default to 1; Describes the axis of the inputs beacuse 0th axis most likely describes the batch size\n",
        "epsilon | (float) default to 0.001. Small value to be added to the stddev to avoid devide by zero\n",
        "***Inputs*** | -\n",
        "input | Input tensor which layer normalization will be applied to\n",
        "***Outputs*** | -\n",
        "output | Normalized values\n",
        "mean | Mean values for each feature vector\n",
        "stddev | Standard deviations for each featur vector\n",
        "\n",
        "\n"
      ]
    }
  ]
}