{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatacrc/Notes/blob/master/MachineLearning/MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTdQ9eOMN339",
        "colab_type": "text"
      },
      "source": [
        "## Machine Learning Interview Questions\n",
        "source: Andrew Ng's Machine Learning https://www.coursera.org/learn/machine-learning\n",
        "\n",
        "**What is Machine Learning?**\n",
        "Arthur Samuel: “The field of study that gives computers the ability to learn without being explicitly programmed.”\n",
        "Tom Mitchell: “A computer program is said to learn from experience E wrt some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”\n",
        "\n",
        "**What are the different classes of Machine Learning?**\n",
        "Supervised, Unsupervised, and Reinforcement Learning\n",
        "\n",
        "*Supervised Learning(SL):* We are given a data set with input and correct output.\n",
        "Regression: map input variables to some continuous function e.g. predict the age of a person\n",
        "Classification: map input variables to into discrete categories e.g.: tumor prediction\n",
        "\n",
        "*Unsupervised Learning(UL):*\n",
        "We can derive the structure from data by clustering the data based on relationships among the variables\n",
        "With UL there is no feedback based on the prediction results\n",
        "Non-clustering: “The cocktail party algorithm”\n",
        "\n",
        "**How do you represent a Model?**\n",
        "\n",
        "Training set with m training examples and n features\n",
        ">$(x_{(j)}^{(i)} ,y_{(j)}^{(i)}); i = 1, …, m; j = 1,...,n$\n",
        "\n",
        ">$X = Y = \\mathbb{R}$\n",
        "\n",
        "*Linear Hypothesis function* h : X -> Y\n",
        ">$h_\\theta(x) =\n",
        " \\begin{pmatrix}\n",
        "  \\theta_0&\\theta_1&\\cdots&\\theta_n\n",
        " \\end{pmatrix}\n",
        " \\begin{pmatrix}\n",
        "  x_{0,0} &\\cdots&x_{0,m} \\\\\n",
        "  x_{1,0} &\\cdots&x_{1,m}  \\\\\n",
        "  \\vdots   &\\vdots&\\vdots \\\\\n",
        "  x_{n,0} &\\cdots&x_{n,m} \\\\ \n",
        " \\end{pmatrix} = \\theta^Tx\n",
        " $\n",
        "\n",
        "*Cost function:*\n",
        "Measure the accuracy of our hypothesis function by using a cost function. The objective is to minimize the cost function.\n",
        "MSE or Squared error function \n",
        ">$J(\\theta) = \\frac{mean\\ of\\ the\\ squares\\ of\\ the\\ error}{2} = \\frac{1}{2m}\\sum\\limits_{i=1}^m(h_\\theta(x_i) - y_i)^2$\n",
        "\n",
        "**What is Gradient Descent?**\n",
        "\n",
        "Gradient descent algorithm helps to estimate the parameters in the hypothesis function. The way we perform this operation is by taking the derivative of the cost function and modify the parameters based on the slope of tangent line to a function (derivative) at that point. It will give us the direction to move and steps down the cost function in the direction of steepest descent.\n",
        ">repeat until convergence: \n",
        "{ \n",
        "$\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,..)$ \n",
        "} \n",
        "\n",
        "where $\\alpha$ = learning rate\n",
        "\n",
        "**What is batch gradient descent?**\n",
        "\n",
        "Batch gradient descent looks at every example in the entire training set.\n",
        "\n",
        "**What is feature scaling?**\n",
        "\n",
        "Feature scaling involves dividing the input values by the range(i.e. the maximum value minus the minimum value), resulting in a new range of just 1.\n",
        "\n",
        "**What is mean normalization?**\n",
        "\n",
        "mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.\n",
        "$x_i := \\frac{x_i - \\mu_i}{s_i}$; $s_i$ = (max - min) or standard deviation.\n",
        "\n",
        "**Why can't you use linear regression for binary classification task?**\n",
        "\n",
        "Classification is not a linear function. \n",
        "\n",
        "**What is the Hypothesis representation for Logistic regression?**\n",
        "$h_\\theta(x)$ give us the probability that our output is 1. This creates a *decision boundary* that separates the area where y = 0 and where y = 1.\n",
        ">$h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1+e^{-(\\theta^Tx)}}$\n",
        "\n",
        "**What is a Cost function for a logistic regression?**\n",
        ">$J(\\theta) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}Cost(h_\\theta(x^{(i)}, y^{(i)})$\n",
        "\n",
        ">$Cost(h_\\theta(x, y) = - log(h_\\theta(x))$ if y = 1\n",
        ">$Cost(h_\\theta(x, y) = - log(1- h_\\theta(x))$ if y = 0\n",
        "\n",
        "> $J(\\theta) = - \\frac{1}{m}\\sum\\limits_{i=1}^{m} [y^{(i)}log(h_\\theta(x)) + (1 - y^{(i)})log(1- h_\\theta(x))]$\n",
        "\n",
        "**How do you perform multiclass classification?**\n",
        "\n",
        "One way to do is to train a logistic regression classifier for each class and pick a class the maximizes the probability.\n",
        "\n",
        "**What is underfitting and overfitting?**\n",
        "\n",
        "Underfit, high bias, happens if the model does not have enough parameters to fit the data. On the other hand if you have a too complex model with many parameters then there is a possibility of fitting the data too well creating high variance or Overfitting.\n",
        "\n",
        "**How do you addreress overfitting?**\n",
        "1. Reduce the number of features\n",
        "> * Manually select which features to keep.\n",
        "> * Use a model selection algorithm.\n",
        "1. Regularization\n",
        "> * reduce the magnitude of parameters $\\theta_j$.\n",
        "> * Regularization works well when we have a lot of slightly useful features.\n",
        "\n",
        "**How regularization parameter used?**\n",
        "\n",
        "The $\\lambda$ regularization parameter determines how much the costs of theta paramertes are inflated. And can be controlled by using an additional term as given below Cost function:\n",
        "\n",
        ">Cost function = $min_\\theta\\frac{1}{2m}\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum\\limits_{j=1}^n\\theta_j^2$\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spcCZs60N1eM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}